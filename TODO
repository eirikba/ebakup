MAJOR FEATURE: Implement sync

I need to implement support for mirroring backups across multiple
storage locations (aka disks). This is the primary robustness feature,
and so needs to be done.

---------------------------------------------------------------------------

MAJOR FEATURE: Implement verify-some

I need to implement the necessary verification support to ensure that
no data in the backup remains unverified for too long. This is a
primary robustness feature, and so needs to be done. "sync" is more
important, but it is the combination of "sync" and this one that makes
the system actually robust.

---------------------------------------------------------------------------

Make Database modifications concurrency-safe

The Database object (or more specifically the ContentInfoFile object)
does not keep the content file locked except just when it is modifying
the file. This is semi-intentional, since otherwise the file would
typically be locked for a long time, which would not be nice. I
believe this is only a problem if two processes try to add items with
the same content id. Both processes will fail to find that content id
in the database and then both will add it. In practice, the only case
where this will realistically happen is if both processes adds items
with the exact same content. In which case the resulting data will be
unambiguously "correct", though not entirely optimal.

I think the right solution here is to add some check for whether the
content file has changed before writing new items to it. Probably just
checking whether the last-modified time stamp has changed will be good
enough. If the file has changed, read it in again.

---------------------------------------------------------------------------

Shadow copies of special files

Currently the code will make shadow copies of every file that has
"content". All regular files should have content, and so should be
correctly created. Most kinds of special files are not (currently)
expected to have content, so those will not be created. However,
special files with "content" (which is really a file-specific
description) will be created as regular files with that content. To
begin with, symbolic links are the only files that have "content".

I'm sure this is not the right behaviour, but I'm not sure what the
right behaviour is.

---------------------------------------------------------------------------

Missing test: BackupStorage._find_duplicate_content_of_file() is
not tested at all.

---------------------------------------------------------------------------

Handle files that change during backup

---------------------------------------------------------------------------

Other performance ideas?

Maybe using multiple threads to read data? This would eliminate the
wait between read commands and could also provide the scheduler with
more options to read more efficiently. (When backing up lots of small
files that already have duplicates in the content store, it seems
there are read() calls that are by far the most expensive ones,
leaving the program running almost entirely in iowait.) A quick test
application that only reads files without doing anything else seems to
have around 2x speedup with this trick (when using around 10 threads).

Try to read files in disk allocation order. Don't know how to figure
that out, but sorting on inode order could be a good approximation.
Same test application as above gave more than 10 times speedup with
this trick (well, sorting on name in this case).


---------------------------------------------------------------------------

Handle failed backups

If a backup does not complete correctly, there will/may be some cruft
left behind. Particularly after other improvements (such as "buffer
database changes").

- Temporary files that haven't been deleted.
  - Temporary add-to-content-store file
  - Backup database ".new" file
  - Half-done shadow tree
- Files in the content store that aren't listed in the content db
- Shadow trees that are missing their backup database

---------------------------------------------------------------------------

Buffer database changes

Instead of writing every change to the database as they are made, I
could buffer up a bunch of changes and write them together. Probably
the best solution here would be to have some in-memory storage of the
data not yet written, and then have a background thread that wakes up
every 5 minutes and writes any buffered data to disk.

---------------------------------------------------------------------------

BackupStorage._make_path_from_contentid() hardcodes the splitting points.

Test: Not created

Each path component (except the last) should have the same length as
all other items in the same directory.


---------------------------------------------------------------------------

Many "problematic" cases not handled

There are plenty of cases that aren't fully handled. In particular
cases that shouldn't happen when everything is correct. Most of these
can be found by searching for NotTestedError.

---------------------------------------------------------------------------
